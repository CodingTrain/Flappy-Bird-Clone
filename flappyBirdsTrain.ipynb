{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flappyBirdsTrain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "7PVx00g90FFN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lq4SQq60Rli",
        "colab_type": "text"
      },
      "source": [
        "# Imports and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLGYBFtJ36ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization, Flatten, Bidirectional\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPC1zsd937r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in frames\n",
        "SEQ_LEN = 100\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 512\n",
        "LOSS =  tf.keras.losses.mse#tf.keras.losses.logcosh\n",
        "LOSS_NAME = \"MSE\"\n",
        "OPT =  tf.keras.optimizers.Adam(lr=LEARNING_RATE, decay=1e-6)\n",
        "NAME = f\"RacingLine-{SEQ_LEN}-EPOCH-{EPOCHS}-LR-{LEARNING_RATE}-BS-{BATCH_SIZE}-LOSS-{LOSS_NAME}\"\n",
        "\n",
        "MAX_X = 15.29\n",
        "MIN_X = -225.62\n",
        "MAX_Y = 77.88\n",
        "MIN_Y = -275.18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PVx00g90FFN",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7b08NnH3-R8",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "def sk_scale_df(_df):\n",
        "    for col in _df.columns:\n",
        "        _df[col] = preprocessing.scale(_df[col].values)#scales values between 0 and 1\n",
        "\n",
        "    _df.dropna(inplace=True)\n",
        "\n",
        "    return _df\n",
        "\n",
        "def min_max_scaling(_df):\n",
        "    #x1 = (x - min_x) / (max_x - min_x)\n",
        "    for col in _df.columns:\n",
        "        max_val = max(_df[col])\n",
        "        min_val = min(_df[col])\n",
        "\n",
        "        _df[col] = (_df[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "    _df.dropna(inplace=True)\n",
        "\n",
        "    return _df\n",
        "\n",
        "def build_seq(_df):\n",
        "    #shape:\n",
        "    #input\n",
        "    #(legth data set, SEQ_LEN, 5)\n",
        "    #output\n",
        "    #(length df, SEQ_LEN, 2)\n",
        "\n",
        "    out_df = _df.copy()\n",
        "\n",
        "    out_df = out_df.drop([\"birdVelocity\", \"pipeY\"], axis=1)#drops all the non x and y columns\n",
        "\n",
        "    sequential_data = []\n",
        "    for i in range(len(_df.values) - (SEQ_LEN + SEQ_LEN)):\n",
        "\n",
        "        prev_end = i+SEQ_LEN;\n",
        "\n",
        "        _inp = _df.values[i:prev_end]\n",
        "        _out = out_df.values[prev_end:prev_end+SEQ_LEN]\n",
        "        \n",
        "        sequential_data.append([np.array(_inp), np.array(_out)])\n",
        "\n",
        "    random.shuffle(sequential_data)\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWri_QCi0f_i",
        "colab_type": "text"
      },
      "source": [
        "# load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02te9ATH4RgN",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "b9562153-b714-4ab7-b462-4bc8ad66055e"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "main_df = pd.read_csv(io.BytesIO(uploaded['flappyBird.csv']))\n",
        "\n",
        "main_df = sk_scale_df(main_df)\n",
        "\n",
        "x_vals = np.linspace(start=0, stop=len(main_df), num=len(main_df))\n",
        "\n",
        "#y vals are inverted as p5 has the origin in the top left corner\n",
        "main_input, main_output = build_seq(main_df)\n",
        "\n",
        "train_input, test_input, train_output, test_output = train_test_split(main_input, main_output, test_size = 0.1, random_state = 0)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a895b7e-b89b-4b10-85d5-7dbe8e30c8cd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4a895b7e-b89b-4b10-85d5-7dbe8e30c8cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving flappyBird.csv to flappyBird (6).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7-4T8cl0lhP",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s49Ojuuh4Uiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48ac7037-ef16-4386-9ea7-22442ad8977c"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(128, input_shape=(train_input.shape[1:]), return_sequences=True, activation=\"tanh\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True, activation=\"tanh\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True, activation=\"tanh\"))\n",
        "\n",
        "model.add(Dense(2,  activation='linear'))\n",
        "\n",
        "model.compile(loss=LOSS, \n",
        "                optimizer=OPT)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_input, train_output,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(test_input, test_output)\n",
        ")\n",
        "\n",
        "pred = model.predict(np.array([test_input[0]]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1581 samples, validate on 176 samples\n",
            "Epoch 1/300\n",
            "1581/1581 [==============================] - 2s 1ms/sample - loss: 0.5189 - val_loss: 0.3610\n",
            "Epoch 2/300\n",
            "1581/1581 [==============================] - 1s 342us/sample - loss: 0.3863 - val_loss: 0.3262\n",
            "Epoch 3/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.3460 - val_loss: 0.4475\n",
            "Epoch 4/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.3646 - val_loss: 0.2980\n",
            "Epoch 5/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.3366 - val_loss: 0.3182\n",
            "Epoch 6/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.3650 - val_loss: 0.2809\n",
            "Epoch 7/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.3458 - val_loss: 0.2984\n",
            "Epoch 8/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.3286 - val_loss: 0.2840\n",
            "Epoch 9/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.3173 - val_loss: 0.2782\n",
            "Epoch 10/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.3328 - val_loss: 0.2795\n",
            "Epoch 11/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.3173 - val_loss: 0.2694\n",
            "Epoch 12/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.3006 - val_loss: 0.2511\n",
            "Epoch 13/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.2810 - val_loss: 0.2735\n",
            "Epoch 14/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.2803 - val_loss: 0.2620\n",
            "Epoch 15/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.2828 - val_loss: 0.2451\n",
            "Epoch 16/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.2602 - val_loss: 0.2441\n",
            "Epoch 17/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.2510 - val_loss: 0.2813\n",
            "Epoch 18/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.2529 - val_loss: 0.2447\n",
            "Epoch 19/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.2238 - val_loss: 0.3262\n",
            "Epoch 20/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.2999 - val_loss: 0.3155\n",
            "Epoch 21/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.3040 - val_loss: 0.2902\n",
            "Epoch 22/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.2787 - val_loss: 0.2727\n",
            "Epoch 23/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.2548 - val_loss: 0.2552\n",
            "Epoch 24/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.2347 - val_loss: 0.2383\n",
            "Epoch 25/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.2434 - val_loss: 0.2607\n",
            "Epoch 26/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.2458 - val_loss: 0.2668\n",
            "Epoch 27/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.2356 - val_loss: 0.2412\n",
            "Epoch 28/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.2196 - val_loss: 0.3111\n",
            "Epoch 29/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.2472 - val_loss: 0.2476\n",
            "Epoch 30/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.2372 - val_loss: 0.2380\n",
            "Epoch 31/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.2221 - val_loss: 0.2383\n",
            "Epoch 32/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.2135 - val_loss: 0.2187\n",
            "Epoch 33/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.2043 - val_loss: 0.2135\n",
            "Epoch 34/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1952 - val_loss: 0.2013\n",
            "Epoch 35/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1919 - val_loss: 0.1927\n",
            "Epoch 36/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1934 - val_loss: 0.2003\n",
            "Epoch 37/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.2051 - val_loss: 0.2149\n",
            "Epoch 38/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.2153 - val_loss: 0.2166\n",
            "Epoch 39/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.2049 - val_loss: 0.2132\n",
            "Epoch 40/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.2042 - val_loss: 0.2090\n",
            "Epoch 41/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1970 - val_loss: 0.2072\n",
            "Epoch 42/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1901 - val_loss: 0.1997\n",
            "Epoch 43/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1863 - val_loss: 0.1951\n",
            "Epoch 44/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1840 - val_loss: 0.1957\n",
            "Epoch 45/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.1832 - val_loss: 0.1908\n",
            "Epoch 46/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1797 - val_loss: 0.1867\n",
            "Epoch 47/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.1757 - val_loss: 0.1827\n",
            "Epoch 48/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1731 - val_loss: 0.1819\n",
            "Epoch 49/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1752 - val_loss: 0.1910\n",
            "Epoch 50/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1849 - val_loss: 0.1907\n",
            "Epoch 51/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1843 - val_loss: 0.1943\n",
            "Epoch 52/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1805 - val_loss: 0.1883\n",
            "Epoch 53/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1769 - val_loss: 0.1862\n",
            "Epoch 54/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1724 - val_loss: 0.1769\n",
            "Epoch 55/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1659 - val_loss: 0.1723\n",
            "Epoch 56/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1617 - val_loss: 0.1682\n",
            "Epoch 57/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1642 - val_loss: 0.1728\n",
            "Epoch 58/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1655 - val_loss: 0.1731\n",
            "Epoch 59/300\n",
            "1581/1581 [==============================] - 1s 331us/sample - loss: 0.1635 - val_loss: 0.1615\n",
            "Epoch 60/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1662 - val_loss: 0.1725\n",
            "Epoch 61/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1669 - val_loss: 0.1606\n",
            "Epoch 62/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1575 - val_loss: 0.1626\n",
            "Epoch 63/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1537 - val_loss: 0.1615\n",
            "Epoch 64/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1612 - val_loss: 0.1555\n",
            "Epoch 65/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1620 - val_loss: 0.1565\n",
            "Epoch 66/300\n",
            "1581/1581 [==============================] - 1s 330us/sample - loss: 0.1570 - val_loss: 0.1437\n",
            "Epoch 67/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1467 - val_loss: 0.1440\n",
            "Epoch 68/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1389 - val_loss: 0.1304\n",
            "Epoch 69/300\n",
            "1581/1581 [==============================] - 1s 319us/sample - loss: 0.1291 - val_loss: 0.1481\n",
            "Epoch 70/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1535 - val_loss: 0.1469\n",
            "Epoch 71/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1525 - val_loss: 0.1554\n",
            "Epoch 72/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1468 - val_loss: 0.1492\n",
            "Epoch 73/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1426 - val_loss: 0.1533\n",
            "Epoch 74/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1542 - val_loss: 0.1381\n",
            "Epoch 75/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1495 - val_loss: 0.1603\n",
            "Epoch 76/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1471 - val_loss: 0.1299\n",
            "Epoch 77/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1301 - val_loss: 0.1219\n",
            "Epoch 78/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1206 - val_loss: 0.1926\n",
            "Epoch 79/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1867 - val_loss: 0.2019\n",
            "Epoch 80/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1931 - val_loss: 0.2060\n",
            "Epoch 81/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1837 - val_loss: 0.1854\n",
            "Epoch 82/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1824 - val_loss: 0.2194\n",
            "Epoch 83/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1899 - val_loss: 0.1809\n",
            "Epoch 84/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1737 - val_loss: 0.1601\n",
            "Epoch 85/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1641 - val_loss: 0.1562\n",
            "Epoch 86/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1548 - val_loss: 0.1398\n",
            "Epoch 87/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.1515 - val_loss: 0.1367\n",
            "Epoch 88/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1480 - val_loss: 0.1420\n",
            "Epoch 89/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1472 - val_loss: 0.1380\n",
            "Epoch 90/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1460 - val_loss: 0.1493\n",
            "Epoch 91/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1415 - val_loss: 0.1354\n",
            "Epoch 92/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1374 - val_loss: 0.1344\n",
            "Epoch 93/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1339 - val_loss: 0.2123\n",
            "Epoch 94/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1974 - val_loss: 0.1841\n",
            "Epoch 95/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1735 - val_loss: 0.1669\n",
            "Epoch 96/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1618 - val_loss: 0.1504\n",
            "Epoch 97/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1511 - val_loss: 0.1384\n",
            "Epoch 98/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1421 - val_loss: 0.1412\n",
            "Epoch 99/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1355 - val_loss: 0.1299\n",
            "Epoch 100/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1345 - val_loss: 0.1447\n",
            "Epoch 101/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1461 - val_loss: 0.1369\n",
            "Epoch 102/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1390 - val_loss: 0.1596\n",
            "Epoch 103/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1570 - val_loss: 0.1449\n",
            "Epoch 104/300\n",
            "1581/1581 [==============================] - 1s 318us/sample - loss: 0.1463 - val_loss: 0.1332\n",
            "Epoch 105/300\n",
            "1581/1581 [==============================] - 1s 318us/sample - loss: 0.1409 - val_loss: 0.1317\n",
            "Epoch 106/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1344 - val_loss: 0.1273\n",
            "Epoch 107/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1359 - val_loss: 0.1252\n",
            "Epoch 108/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1307 - val_loss: 0.1188\n",
            "Epoch 109/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1274 - val_loss: 0.1162\n",
            "Epoch 110/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1285 - val_loss: 0.1149\n",
            "Epoch 111/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1222 - val_loss: 0.1442\n",
            "Epoch 112/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1470 - val_loss: 0.1498\n",
            "Epoch 113/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1465 - val_loss: 0.1409\n",
            "Epoch 114/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1427 - val_loss: 0.1424\n",
            "Epoch 115/300\n",
            "1581/1581 [==============================] - 1s 330us/sample - loss: 0.1413 - val_loss: 0.1884\n",
            "Epoch 116/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1665 - val_loss: 0.1585\n",
            "Epoch 117/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1549 - val_loss: 0.1477\n",
            "Epoch 118/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1442 - val_loss: 0.1571\n",
            "Epoch 119/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1467 - val_loss: 0.1528\n",
            "Epoch 120/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1446 - val_loss: 0.1435\n",
            "Epoch 121/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1388 - val_loss: 0.1363\n",
            "Epoch 122/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1337 - val_loss: 0.1294\n",
            "Epoch 123/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1242 - val_loss: 0.1189\n",
            "Epoch 124/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1191 - val_loss: 0.1239\n",
            "Epoch 125/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1159 - val_loss: 0.1344\n",
            "Epoch 126/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1300 - val_loss: 0.1221\n",
            "Epoch 127/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1235 - val_loss: 0.1202\n",
            "Epoch 128/300\n",
            "1581/1581 [==============================] - 1s 332us/sample - loss: 0.1163 - val_loss: 0.1107\n",
            "Epoch 129/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1098 - val_loss: 0.1065\n",
            "Epoch 130/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1046 - val_loss: 0.1028\n",
            "Epoch 131/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1080 - val_loss: 0.1165\n",
            "Epoch 132/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1226 - val_loss: 0.1200\n",
            "Epoch 133/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1196 - val_loss: 0.1070\n",
            "Epoch 134/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1093 - val_loss: 0.1254\n",
            "Epoch 135/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1103 - val_loss: 0.1226\n",
            "Epoch 136/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1163 - val_loss: 0.1030\n",
            "Epoch 137/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1030 - val_loss: 0.1099\n",
            "Epoch 138/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1028 - val_loss: 0.1044\n",
            "Epoch 139/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1081 - val_loss: 0.0982\n",
            "Epoch 140/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0952 - val_loss: 0.0928\n",
            "Epoch 141/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0881 - val_loss: 0.0968\n",
            "Epoch 142/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0893 - val_loss: 0.0917\n",
            "Epoch 143/300\n",
            "1581/1581 [==============================] - 1s 319us/sample - loss: 0.0851 - val_loss: 0.0815\n",
            "Epoch 144/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.0872 - val_loss: 0.0836\n",
            "Epoch 145/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0810 - val_loss: 0.0760\n",
            "Epoch 146/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0777 - val_loss: 0.0798\n",
            "Epoch 147/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0757 - val_loss: 0.0724\n",
            "Epoch 148/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0789 - val_loss: 0.0725\n",
            "Epoch 149/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0760 - val_loss: 0.0695\n",
            "Epoch 150/300\n",
            "1581/1581 [==============================] - 1s 319us/sample - loss: 0.0745 - val_loss: 0.0663\n",
            "Epoch 151/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0706 - val_loss: 0.0666\n",
            "Epoch 152/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0648 - val_loss: 0.0612\n",
            "Epoch 153/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.0684 - val_loss: 0.0668\n",
            "Epoch 154/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1011 - val_loss: 0.1207\n",
            "Epoch 155/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1116 - val_loss: 0.1052\n",
            "Epoch 156/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1052 - val_loss: 0.1014\n",
            "Epoch 157/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0987 - val_loss: 0.0958\n",
            "Epoch 158/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0961 - val_loss: 0.0890\n",
            "Epoch 159/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0893 - val_loss: 0.0892\n",
            "Epoch 160/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0845 - val_loss: 0.0808\n",
            "Epoch 161/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0813 - val_loss: 0.0793\n",
            "Epoch 162/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0851 - val_loss: 0.0819\n",
            "Epoch 163/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0815 - val_loss: 0.0816\n",
            "Epoch 164/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.0786 - val_loss: 0.0793\n",
            "Epoch 165/300\n",
            "1581/1581 [==============================] - 1s 330us/sample - loss: 0.0840 - val_loss: 0.1014\n",
            "Epoch 166/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1131 - val_loss: 0.0946\n",
            "Epoch 167/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1021 - val_loss: 0.1044\n",
            "Epoch 168/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1078 - val_loss: 0.0991\n",
            "Epoch 169/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1141 - val_loss: 0.1030\n",
            "Epoch 170/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0984 - val_loss: 0.0849\n",
            "Epoch 171/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0860 - val_loss: 0.0958\n",
            "Epoch 172/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0789 - val_loss: 0.0761\n",
            "Epoch 173/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0858 - val_loss: 0.0682\n",
            "Epoch 174/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0764 - val_loss: 0.0929\n",
            "Epoch 175/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0732 - val_loss: 0.1101\n",
            "Epoch 176/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0990 - val_loss: 0.0944\n",
            "Epoch 177/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0825 - val_loss: 0.0706\n",
            "Epoch 178/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0748 - val_loss: 0.0799\n",
            "Epoch 179/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0713 - val_loss: 0.0659\n",
            "Epoch 180/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.0665 - val_loss: 0.0861\n",
            "Epoch 181/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0725 - val_loss: 0.0706\n",
            "Epoch 182/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0615 - val_loss: 0.0580\n",
            "Epoch 183/300\n",
            "1581/1581 [==============================] - 1s 330us/sample - loss: 0.0608 - val_loss: 0.1099\n",
            "Epoch 184/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1571 - val_loss: 0.1346\n",
            "Epoch 185/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1287 - val_loss: 0.1196\n",
            "Epoch 186/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1136 - val_loss: 0.1087\n",
            "Epoch 187/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1039 - val_loss: 0.0949\n",
            "Epoch 188/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0991 - val_loss: 0.0971\n",
            "Epoch 189/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0902 - val_loss: 0.0853\n",
            "Epoch 190/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0907 - val_loss: 0.0873\n",
            "Epoch 191/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0886 - val_loss: 0.0951\n",
            "Epoch 192/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0925 - val_loss: 0.0847\n",
            "Epoch 193/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0865 - val_loss: 0.0968\n",
            "Epoch 194/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1178 - val_loss: 0.1494\n",
            "Epoch 195/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1249 - val_loss: 0.1338\n",
            "Epoch 196/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1164 - val_loss: 0.1092\n",
            "Epoch 197/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1033 - val_loss: 0.0980\n",
            "Epoch 198/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.0905 - val_loss: 0.1150\n",
            "Epoch 199/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1267 - val_loss: 0.0997\n",
            "Epoch 200/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.1120 - val_loss: 0.0925\n",
            "Epoch 201/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.1033 - val_loss: 0.0966\n",
            "Epoch 202/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0951 - val_loss: 0.0877\n",
            "Epoch 203/300\n",
            "1581/1581 [==============================] - 1s 318us/sample - loss: 0.0877 - val_loss: 0.0832\n",
            "Epoch 204/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0800 - val_loss: 0.0828\n",
            "Epoch 205/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0745 - val_loss: 0.0663\n",
            "Epoch 206/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0682 - val_loss: 0.1394\n",
            "Epoch 207/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1272 - val_loss: 0.1524\n",
            "Epoch 208/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.1397 - val_loss: 0.1335\n",
            "Epoch 209/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1219 - val_loss: 0.1059\n",
            "Epoch 210/300\n",
            "1581/1581 [==============================] - 1s 319us/sample - loss: 0.1233 - val_loss: 0.1037\n",
            "Epoch 211/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1231 - val_loss: 0.1250\n",
            "Epoch 212/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1153 - val_loss: 0.1066\n",
            "Epoch 213/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1064 - val_loss: 0.0920\n",
            "Epoch 214/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0967 - val_loss: 0.1300\n",
            "Epoch 215/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1335 - val_loss: 0.1300\n",
            "Epoch 216/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1213 - val_loss: 0.1291\n",
            "Epoch 217/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1195 - val_loss: 0.1136\n",
            "Epoch 218/300\n",
            "1581/1581 [==============================] - 1s 319us/sample - loss: 0.1090 - val_loss: 0.1088\n",
            "Epoch 219/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1067 - val_loss: 0.1069\n",
            "Epoch 220/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.1018 - val_loss: 0.0999\n",
            "Epoch 221/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0959 - val_loss: 0.0971\n",
            "Epoch 222/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0938 - val_loss: 0.0937\n",
            "Epoch 223/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0926 - val_loss: 0.0920\n",
            "Epoch 224/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0863 - val_loss: 0.0910\n",
            "Epoch 225/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0854 - val_loss: 0.1034\n",
            "Epoch 226/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.1077 - val_loss: 0.1039\n",
            "Epoch 227/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0997 - val_loss: 0.1242\n",
            "Epoch 228/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.1297 - val_loss: 0.1191\n",
            "Epoch 229/300\n",
            "1581/1581 [==============================] - 1s 330us/sample - loss: 0.1183 - val_loss: 0.1131\n",
            "Epoch 230/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.1126 - val_loss: 0.0979\n",
            "Epoch 231/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.1038 - val_loss: 0.1046\n",
            "Epoch 232/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.1044 - val_loss: 0.1000\n",
            "Epoch 233/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0973 - val_loss: 0.0923\n",
            "Epoch 234/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0952 - val_loss: 0.0893\n",
            "Epoch 235/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0893 - val_loss: 0.0834\n",
            "Epoch 236/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0862 - val_loss: 0.0791\n",
            "Epoch 237/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0814 - val_loss: 0.0861\n",
            "Epoch 238/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0810 - val_loss: 0.0738\n",
            "Epoch 239/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0771 - val_loss: 0.0898\n",
            "Epoch 240/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0863 - val_loss: 0.0750\n",
            "Epoch 241/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0773 - val_loss: 0.0689\n",
            "Epoch 242/300\n",
            "1581/1581 [==============================] - 1s 329us/sample - loss: 0.0818 - val_loss: 0.0923\n",
            "Epoch 243/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0809 - val_loss: 0.0719\n",
            "Epoch 244/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0693 - val_loss: 0.0880\n",
            "Epoch 245/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0842 - val_loss: 0.1048\n",
            "Epoch 246/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.1036 - val_loss: 0.0784\n",
            "Epoch 247/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0808 - val_loss: 0.0735\n",
            "Epoch 248/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0843 - val_loss: 0.0843\n",
            "Epoch 249/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0808 - val_loss: 0.0662\n",
            "Epoch 250/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0730 - val_loss: 0.0673\n",
            "Epoch 251/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0675 - val_loss: 0.0565\n",
            "Epoch 252/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0608 - val_loss: 0.0587\n",
            "Epoch 253/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0566 - val_loss: 0.0654\n",
            "Epoch 254/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0540 - val_loss: 0.0528\n",
            "Epoch 255/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0486 - val_loss: 0.0813\n",
            "Epoch 256/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.0657 - val_loss: 0.0552\n",
            "Epoch 257/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0633 - val_loss: 0.0687\n",
            "Epoch 258/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0602 - val_loss: 0.0522\n",
            "Epoch 259/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0541 - val_loss: 0.0515\n",
            "Epoch 260/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0491 - val_loss: 0.0480\n",
            "Epoch 261/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0478 - val_loss: 0.0478\n",
            "Epoch 262/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0464 - val_loss: 0.0494\n",
            "Epoch 263/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0445 - val_loss: 0.0448\n",
            "Epoch 264/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0456 - val_loss: 0.0476\n",
            "Epoch 265/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0442 - val_loss: 0.0403\n",
            "Epoch 266/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0418 - val_loss: 0.0389\n",
            "Epoch 267/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0426 - val_loss: 0.0369\n",
            "Epoch 268/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0365 - val_loss: 0.0396\n",
            "Epoch 269/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0373 - val_loss: 0.0357\n",
            "Epoch 270/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.0347 - val_loss: 0.0419\n",
            "Epoch 271/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0427 - val_loss: 0.0344\n",
            "Epoch 272/300\n",
            "1581/1581 [==============================] - 1s 320us/sample - loss: 0.0377 - val_loss: 0.0761\n",
            "Epoch 273/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0638 - val_loss: 0.0754\n",
            "Epoch 274/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0876 - val_loss: 0.0865\n",
            "Epoch 275/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0836 - val_loss: 0.0754\n",
            "Epoch 276/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0774 - val_loss: 0.0708\n",
            "Epoch 277/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0692 - val_loss: 0.0634\n",
            "Epoch 278/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0629 - val_loss: 0.0568\n",
            "Epoch 279/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0548 - val_loss: 0.0588\n",
            "Epoch 280/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0516 - val_loss: 0.0528\n",
            "Epoch 281/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0431 - val_loss: 0.0435\n",
            "Epoch 282/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0403 - val_loss: 0.0371\n",
            "Epoch 283/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0490 - val_loss: 0.2133\n",
            "Epoch 284/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.1439 - val_loss: 0.1193\n",
            "Epoch 285/300\n",
            "1581/1581 [==============================] - 1s 332us/sample - loss: 0.1107 - val_loss: 0.0943\n",
            "Epoch 286/300\n",
            "1581/1581 [==============================] - 1s 321us/sample - loss: 0.0868 - val_loss: 0.0799\n",
            "Epoch 287/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0780 - val_loss: 0.0823\n",
            "Epoch 288/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0755 - val_loss: 0.0731\n",
            "Epoch 289/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0671 - val_loss: 0.0698\n",
            "Epoch 290/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0578 - val_loss: 0.0576\n",
            "Epoch 291/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0527 - val_loss: 0.0569\n",
            "Epoch 292/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0580 - val_loss: 0.0696\n",
            "Epoch 293/300\n",
            "1581/1581 [==============================] - 1s 328us/sample - loss: 0.0540 - val_loss: 0.0640\n",
            "Epoch 294/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0586 - val_loss: 0.0547\n",
            "Epoch 295/300\n",
            "1581/1581 [==============================] - 1s 324us/sample - loss: 0.0647 - val_loss: 0.0505\n",
            "Epoch 296/300\n",
            "1581/1581 [==============================] - 1s 322us/sample - loss: 0.0611 - val_loss: 0.0700\n",
            "Epoch 297/300\n",
            "1581/1581 [==============================] - 1s 323us/sample - loss: 0.0713 - val_loss: 0.0740\n",
            "Epoch 298/300\n",
            "1581/1581 [==============================] - 1s 325us/sample - loss: 0.0578 - val_loss: 0.0716\n",
            "Epoch 299/300\n",
            "1581/1581 [==============================] - 1s 326us/sample - loss: 0.0536 - val_loss: 0.0559\n",
            "Epoch 300/300\n",
            "1581/1581 [==============================] - 1s 327us/sample - loss: 0.0552 - val_loss: 0.0632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZQg9fk0poo",
        "colab_type": "text"
      },
      "source": [
        "# Plot predicition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8IZqkTg4Z_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6a058291-c4f7-4105-8aea-3432b8149a67"
      },
      "source": [
        "x_in_seq_vals = np.linspace(start=0, stop=SEQ_LEN, num=SEQ_LEN)\n",
        "x_out_seq_vals = np.linspace(start=SEQ_LEN, stop=2*SEQ_LEN, num=SEQ_LEN)\n",
        "\n",
        "y_inp = []\n",
        "y_out = []\n",
        "y_pred = []\n",
        "\n",
        "\n",
        "for i in range(SEQ_LEN):\n",
        "    y_inp.append(test_input[0][i][0])\n",
        "    y_out.append(test_output[0][i][0])\n",
        "    y_pred.append(pred[0][i][0])\n",
        "\n",
        "plt.scatter(x_in_seq_vals, y_inp, color=\"red\", label=\"input\")\n",
        "plt.scatter(x_out_seq_vals, y_out, color=\"blue\", label=\"output\")\n",
        "plt.scatter(x_out_seq_vals, y_pred, color=\"green\", label=\"output\")\n",
        "\n",
        "#plt.scatter(x_vals, main_df[\"birdY\"], color=\"green\")\n",
        "#plt.scatter(x_vals, main_df[\"pipeY\"], color=\"black\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUeGuKIpSQP4/+NS2Zh7/sjsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE8wxBM88QiK",
        "colab_type": "text"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbmJ3twe8S8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "fd2b3668-ed99-4911-e127-13ab92727c6f"
      },
      "source": [
        "model.save('keras.h5')\n",
        "!pip install tensorflowjs \n",
        "!mkdir model\n",
        "!tensorflowjs_converter --input_format keras keras.h5 model/\n",
        "!zip -r model.zip model \n",
        "from google.colab import files\n",
        "files.download('model.zip')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.6/dist-packages (1.2.6)\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.16.4)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.2.4)\n",
            "Requirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.5.0->tensorflowjs) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (0.33.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->tensorflowjs) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.5.0->tensorflowjs) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->tensorflowjs) (0.15.5)\n",
            "mkdir: cannot create directory ‘model’: File exists\n",
            "updating: model/ (stored 0%)\n",
            "updating: model/group1-shard1of1.bin (deflated 7%)\n",
            "updating: model/model.json (deflated 82%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}